{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "479f0146-2ed1-4305-bfb1-9a4a6278b8aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the full dataset...\n",
      "dataframe from: 2025-04-01 00:00:00 to: 2025-05-20 23:59:59\n",
      "Full dataset loaded. Shape: (75432606, 22)\n",
      " Estimated size of df in MB:  12661.11245727539\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 153\u001b[0m\n\u001b[1;32m    151\u001b[0m num_chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m    152\u001b[0m chunk_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(df) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m num_chunks \u001b[38;5;66;03m# Use integer division\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m processed_data_dir \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;18;43m__file__\u001b[39;49m)\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchunk_data\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28mprint\u001b[39m(processed_data_dir)\n\u001b[1;32m    155\u001b[0m processed_data_dir\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import sys\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import KFold\n",
    "from xgboost import XGBRegressor\n",
    "import gc\n",
    "\n",
    "project_root = Path().resolve().parent\n",
    "sys.path.insert(0, str(project_root / \"src\"))\n",
    "\n",
    "from dspy.hdb import get_dataset\n",
    "from dspy.sim.market_simulator import MarketSimulator\n",
    "from dspy.utils import to_ns, ts_to_str\n",
    "from dspy.features.feature_utils import apply_batch_features, extract_features , flatten_features\n",
    "from dspy.agents.agent_utils import get_agent\n",
    "from dspy.features.utils import get_products\n",
    "from dspy.utils import add_ts_dt\n",
    "\n",
    "\n",
    "\n",
    "# ---------- Load run config file ----------\n",
    "\n",
    "def load_config(path: Path) -> dict:\n",
    "    with open(path, \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "config_path = project_root / \"run/run_config.json\"\n",
    "config = load_config(config_path)\n",
    "\n",
    "dataset_name     = config[\"dataset\"]\n",
    "product          = config[\"product\"]\n",
    "depth            = config[\"depth\"]\n",
    "latency_ns       = config[\"latency_micros\"] * 1_000\n",
    "max_inventory    = config[\"max_inventory\"]\n",
    "inv_penalty      = config[\"inventory_penalty\"]\n",
    "initial_cash     = config[\"initial_cash\"]\n",
    "agent_config     = config[\"agent\"]\n",
    "intervals        = config[\"intervals\"]\n",
    "min_order_size   = config[\"min_order_size\"]\n",
    "tick_size        = config[\"tick_size\"]\n",
    "initial_cash     = config[\"initial_cash\"]\n",
    "cost_in_bps      = config[\"cost_in_bps\"]\n",
    "fixed_cost       = config[\"fixed_cost\"]\n",
    "simulator_mode   = config[\"simulator_mode\"]\n",
    "system_pc        = config[\"system_pc\"]\n",
    "\n",
    "loader = get_dataset(dataset_name)\n",
    "all_books, all_ts = [], []\n",
    "feature_path = project_root / \"run/features.json\"\n",
    "feature_config = load_config(feature_path)\n",
    "inventory_feature_flag = \"inventory\" in feature_config.keys()\n",
    "\n",
    "\n",
    "from dspy.features.book_features import add_mid,add_vwap,add_ts_dt\n",
    "# df=add_mid(df)\n",
    "# df.head()\n",
    "\n",
    "########\n",
    "\n",
    "\n",
    "#function to add targets\n",
    "def add_target_ret_time(\n",
    "    df: pl.DataFrame,\n",
    "    delta: int = 50,  # in milliseconds\n",
    "    base_col: str = \"mid\",  # 'mid' or 'vwap'\n",
    "    levels: int = 1,\n",
    "    depth: int = 1,\n",
    "    time_col: str = \"ts_dt\",\n",
    "    products: list[str] | None = None\n",
    ") -> pl.DataFrame:\n",
    "\n",
    "    if time_col not in df.columns:\n",
    "        df = add_ts_dt(df)\n",
    "\n",
    "    # Ensure time_col is datetime[ns]\n",
    "    if df[time_col].dtype != pl.Datetime(\"ns\"):\n",
    "        df = df.with_columns(pl.col(time_col).cast(pl.Datetime(\"ns\")))\n",
    "\n",
    "    # Ensure price column exists\n",
    "    if base_col == \"mid\":\n",
    "        col_prefix = \"mid\"\n",
    "        if col_prefix not in df.columns:\n",
    "            df = add_mid(df, products=products)\n",
    "    elif base_col == \"vwap\":\n",
    "        col_prefix = f\"vwap_level{levels}\"\n",
    "        if col_prefix not in df.columns:\n",
    "            df = add_vwap(df, levels=levels, depth=depth, products=products)\n",
    "    else:\n",
    "        raise ValueError(\"base_col must be 'mid' or 'vwap'.\")\n",
    "\n",
    "    \n",
    "\n",
    "    price_col = col_prefix\n",
    "    future_df = (\n",
    "        df.select([time_col, price_col])\n",
    "        .with_columns(\n",
    "            (pl.col(time_col) + pl.duration(milliseconds=-delta))\n",
    "            .cast(pl.Datetime(\"ns\"))\n",
    "            .alias(time_col)\n",
    "        )\n",
    "        .rename({price_col: f\"{price_col}_fut\"})\n",
    "    )\n",
    "    ret_col_name = (\n",
    "            f\"ret_{delta}ms_fut\"\n",
    "        )\n",
    "    df = df.join_asof(\n",
    "        future_df,\n",
    "        left_on=time_col,\n",
    "        right_on=time_col,\n",
    "        strategy=\"backward\",\n",
    "        tolerance=timedelta(milliseconds=1000000)  # generous tolerance\n",
    "    ).with_columns([\n",
    "        (pl.col(f\"{price_col}_fut\")/ (pl.col(price_col) ) - 1).alias(ret_col_name)\n",
    "    ]).drop([f\"{price_col}_fut\"])\n",
    "\n",
    "\n",
    "    return df.drop_nulls()\n",
    "\n",
    "\n",
    "# --- All your existing function definitions and config loading can stay the same ---\n",
    "# correlation_xgb_feature_selection(), add_target_ret_time(), etc.\n",
    "\n",
    "# --- MAIN EXECUTION LOGIC (REVISED WITH YOUR CHUNKING STRATEGY) ---\n",
    "\n",
    "# 1. Load the single, massive interval as you are doing now\n",
    "print('Loading the full dataset...')\n",
    "# ... (your existing code to load the single interval into `df`) ...\n",
    "for interval in intervals:\n",
    "        start_str = interval[\"start\"]\n",
    "        end_str   = interval[\"end\"]\n",
    "        print('dataframe from:', start_str,'to:',end_str)\n",
    "\n",
    "        start_ts = datetime.strptime(interval[\"start\"], \"%Y-%m-%d %H:%M:%S\").strftime(\"%y%m%d.%H%M%S\")\n",
    "        end_ts   = datetime.strptime(interval[\"end\"],   \"%Y-%m-%d %H:%M:%S\").strftime(\"%y%m%d.%H%M%S\")\n",
    "\n",
    "        df = loader.load_book(\n",
    "            product=product,\n",
    "            times=[start_ts, end_ts],\n",
    "            depth=depth,\n",
    "            type=\"book_snapshot_25\",\n",
    "            lazy=False\n",
    "        )\n",
    "print('Full dataset loaded. Shape:', df.shape)\n",
    "print( ' Estimated size of df in MB: ', df.estimated_size(\"mb\"))\n",
    "\n",
    "# 2. Define the number of chunks and the directory for processed files\n",
    "num_chunks = 10\n",
    "chunk_size = len(df) // num_chunks # Use integer division\n",
    "processed_data_dir = Path(__file__).parent/ \"chunk_data\"\n",
    "print(processed_data_dir)\n",
    "processed_data_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Splitting the DataFrame into {num_chunks} chunks of approximately {chunk_size} rows each.\")\n",
    "\n",
    "# 3. Loop through each chunk, process it, and save it to disk\n",
    "feature_cols = []\n",
    "target_list = []\n",
    "for i in range(num_chunks):\n",
    "    print(f\"--- Processing chunk {i+1}/{num_chunks} ---\")\n",
    "    \n",
    "    # Calculate the start offset for the slice\n",
    "    offset = i * chunk_size\n",
    "    \n",
    "    # Get the current chunk using a simple slice\n",
    "    df_chunk = df.slice(offset, chunk_size)\n",
    "\n",
    "    # Apply feature engineering to the chunk\n",
    "    print(f\"  - Applying batch features...\")\n",
    "    df_processed_chunk, feature_cols = apply_batch_features(df_chunk, feature_config)\n",
    "    feature_cols = [col for col in feature_cols if df_processed_chunk[col].dtype != pl.Datetime]\n",
    "    \n",
    "    # Add target variables to the chunk\n",
    "    print(f\"  - Adding targets...\")\n",
    "    time_horizons = [50, 500, 5000]\n",
    "    price = 'mid'\n",
    "    for t in time_horizons:\n",
    "        if i==0:\n",
    "            target_list.append(f\"ret_{t}ms_fut\")\n",
    "        df_processed_chunk = add_target_ret_time(df_processed_chunk, t, price)\n",
    "\n",
    "    # Save the processed chunk to disk\n",
    "    output_path = processed_data_dir / f\"processed_chunk_{i}.parquet\"\n",
    "    print(f\"  - Saving processed chunk to {output_path}\")\n",
    "    df_processed_chunk.write_parquet(output_path)\n",
    "\n",
    "# 4. Release memory of the huge raw DataFrame\n",
    "print(\"\\nReleasing memory of the original raw DataFrame...\")\n",
    "del df\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "# 5. Load all the processed chunks from disk\n",
    "print(\"--- Loading all processed chunks from disk ---\")\n",
    "all_processed_files = list(processed_data_dir.glob(\"*.parquet\"))\n",
    "# df = pl.read_parquet(all_processed_files)\n",
    "df = pl.scan_parquet(all_processed_files) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c41506-bc09-44db-bcbe-9c6445e9aa36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
